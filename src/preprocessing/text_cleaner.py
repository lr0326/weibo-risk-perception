"""
æ–‡æœ¬æ¸…æ´—æ¨¡å—
æä¾›ä¸­æ–‡æ–‡æœ¬çš„æ¸…æ´—ã€æ ‡å‡†åŒ–ç­‰é¢„å¤„ç†åŠŸèƒ½
"""

import re
import unicodedata
from typing import List, Optional, Set

import jieba
import pandas as pd
try:
    from loguru import logger
except ImportError:
    import logging
    logger = logging.getLogger("weibo_risk")

from src.utils.helpers import load_config


class TextCleaner:
    """
    æ–‡æœ¬æ¸…æ´—å™¨

    æ”¯æŒåŠŸèƒ½ï¼š
    - URLç§»é™¤
    - @æåŠç§»é™¤
    - è¯é¢˜æ ‡ç­¾å¤„ç†
    - è¡¨æƒ…ç¬¦å·å¤„ç†
    - ä¸­æ–‡åˆ†è¯
    - åœç”¨è¯è¿‡æ»¤
    """

    def __init__(self, config_path: str = "config/config.yaml"):
        """
        åˆå§‹åŒ–æ–‡æœ¬æ¸…æ´—å™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = load_config(config_path)

        # æ¸…æ´—é…ç½®
        clean_config = self.config.get("preprocessing", {}).get("text_cleaning", {})
        self.remove_urls = clean_config.get("remove_urls", True)
        self.remove_mentions = clean_config.get("remove_mentions", True)
        self.remove_hashtags = clean_config.get("remove_hashtags", False)
        self.remove_emojis = clean_config.get("remove_emojis", False)
        self.remove_punctuation = clean_config.get("remove_punctuation", False)
        self.convert_lowercase = clean_config.get("convert_lowercase", False)
        self.min_length = clean_config.get("min_length", 5)

        # åˆ†è¯é…ç½®
        token_config = self.config.get("preprocessing", {}).get("tokenization", {})
        self.tokenizer_engine = token_config.get("engine", "jieba")
        user_dict_path = token_config.get("user_dict", "")
        stop_words_path = token_config.get("stop_words", "")

        # åŠ è½½ç”¨æˆ·è¯å…¸
        if user_dict_path:
            self._load_user_dict(user_dict_path)

        # åŠ è½½åœç”¨è¯
        self.stop_words: Set[str] = set()
        if stop_words_path:
            self._load_stop_words(stop_words_path)
        else:
            self._load_default_stop_words()

        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self._compile_patterns()

        logger.info("æ–‡æœ¬æ¸…æ´—å™¨åˆå§‹åŒ–å®Œæˆ")

    def _compile_patterns(self):
        """ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        # URLæ¨¡å¼
        self.url_pattern = re.compile(
            r'https?://[^\s<>"{}|\\^`\[\]]+'
            r'|www\.[^\s<>"{}|\\^`\[\]]+'
        )

        # @æåŠæ¨¡å¼
        self.mention_pattern = re.compile(r'@[\w\u4e00-\u9fff]+')

        # è¯é¢˜æ ‡ç­¾æ¨¡å¼ (#è¯é¢˜#)
        self.hashtag_pattern = re.compile(r'#[^#]+#')

        # è¡¨æƒ…ç¬¦å·æ¨¡å¼ [è¡¨æƒ…]
        self.emoji_pattern = re.compile(r'\[[\w\u4e00-\u9fff]+\]')

        # å¤šä½™ç©ºç™½å­—ç¬¦
        self.whitespace_pattern = re.compile(r'\s+')

        # ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
        self.chinese_punctuation = re.compile(
            r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ã€ã€‘ã€Šã€‹ï¼ˆï¼‰â€¦â€”ï½Â·]'
        )

        # éä¸­æ–‡è‹±æ–‡æ•°å­—å­—ç¬¦
        self.invalid_chars = re.compile(
            r'[^\u4e00-\u9fff\u0041-\u005a\u0061-\u007a\u0030-\u0039\s]'
        )

    def _load_user_dict(self, path: str):
        """åŠ è½½ç”¨æˆ·è‡ªå®šä¹‰è¯å…¸"""
        try:
            jieba.load_userdict(path)
            logger.info(f"åŠ è½½ç”¨æˆ·è¯å…¸: {path}")
        except FileNotFoundError:
            logger.warning(f"ç”¨æˆ·è¯å…¸ä¸å­˜åœ¨: {path}")
        except Exception as e:
            logger.error(f"åŠ è½½ç”¨æˆ·è¯å…¸å¤±è´¥: {e}")

    def _load_stop_words(self, path: str):
        """åŠ è½½åœç”¨è¯è¡¨"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                self.stop_words = set(line.strip() for line in f if line.strip())
            logger.info(f"åŠ è½½åœç”¨è¯ {len(self.stop_words)} ä¸ª")
        except FileNotFoundError:
            logger.warning(f"åœç”¨è¯æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            self._load_default_stop_words()
        except Exception as e:
            logger.error(f"åŠ è½½åœç”¨è¯å¤±è´¥: {e}")

    def _load_default_stop_words(self):
        """åŠ è½½é»˜è®¤åœç”¨è¯"""
        default_stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½',
            'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š',
            'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»–', 'å¥¹', 'å®ƒ',
            'ä»¬', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å› ä¸º', 'æ‰€ä»¥',
            'ä½†æ˜¯', 'ç„¶å', 'å¦‚æœ', 'å¯ä»¥', 'æ²¡', 'å—', 'å‘¢', 'å§', 'å•Š',
            'å“¦', 'å—¯', 'å‘€', 'å“ˆ', 'å“', 'å”‰', 'å˜¿', 'å–‚', 'è¯¶'
        }
        self.stop_words = default_stop_words
        logger.info(f"ä½¿ç”¨é»˜è®¤åœç”¨è¯ {len(self.stop_words)} ä¸ª")

    def clean(self, text: str) -> str:
        """
        æ¸…æ´—å•æ¡æ–‡æœ¬

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not text or not isinstance(text, str):
            return ""

        # ç§»é™¤URL
        if self.remove_urls:
            text = self.url_pattern.sub('', text)

        # ç§»é™¤@æåŠ
        if self.remove_mentions:
            text = self.mention_pattern.sub('', text)

        # å¤„ç†è¯é¢˜æ ‡ç­¾
        if self.remove_hashtags:
            text = self.hashtag_pattern.sub('', text)
        else:
            # ä¿ç•™è¯é¢˜å†…å®¹ï¼Œç§»ï¿½ï¿½#ç¬¦å·
            text = re.sub(r'#([^#]+)#', r'\1', text)

        # å¤„ç†è¡¨æƒ…ç¬¦å·
        if self.remove_emojis:
            text = self.emoji_pattern.sub('', text)

        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        if self.remove_punctuation:
            text = self.chinese_punctuation.sub('', text)
            text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)

        # è½¬å°å†™
        if self.convert_lowercase:
            text = text.lower()

        # Unicodeæ ‡å‡†åŒ–
        text = unicodedata.normalize('NFKC', text)

        # å‹ç¼©ç©ºç™½å­—ç¬¦
        text = self.whitespace_pattern.sub(' ', text)

        # å»é™¤é¦–å°¾ç©ºç™½
        text = text.strip()

        return text

    def tokenize(self, text: str, remove_stop_words: bool = True) -> List[str]:
        """
        åˆ†è¯

        Args:
            text: è¾“å…¥æ–‡æœ¬
            remove_stop_words: æ˜¯å¦ç§»é™¤åœç”¨è¯

        Returns:
            åˆ†è¯ç»“æœåˆ—è¡¨
        """
        if not text:
            return []

        # å…ˆæ¸…æ´—
        cleaned = self.clean(text)

        # åˆ†è¯
        if self.tokenizer_engine == "jieba":
            tokens = list(jieba.cut(cleaned))
        else:
            # é»˜è®¤ä½¿ç”¨jieba
            tokens = list(jieba.cut(cleaned))

        # è¿‡æ»¤
        filtered = []
        for token in tokens:
            token = token.strip()

            # è·³è¿‡ç©ºç™½
            if not token:
                continue

            # è·³è¿‡è¿‡çŸ­çš„è¯
            if len(token) < 2 and not token.isdigit():
                continue

            # ç§»é™¤åœç”¨è¯
            if remove_stop_words and token in self.stop_words:
                continue

            filtered.append(token)

        return filtered

    def process_dataframe(
        self,
        df: pd.DataFrame,
        text_column: str = "content",
        output_column: str = "cleaned_content",
        tokenize: bool = False,
        token_column: str = "tokens"
    ) -> pd.DataFrame:
        """
        æ‰¹é‡å¤„ç†DataFrameä¸­çš„æ–‡æœ¬

        Args:
            df: è¾“å…¥DataFrame
            text_column: æ–‡æœ¬åˆ—å
            output_column: è¾“å‡ºåˆ—å
            tokenize: æ˜¯å¦è¿›è¡Œåˆ†è¯
            token_column: åˆ†è¯ç»“æœåˆ—å

        Returns:
            å¤„ç†åçš„DataFrame
        """
        df = df.copy()

        # æ¸…æ´—
        df[output_column] = df[text_column].apply(self.clean)

        # è¿‡æ»¤è¿‡çŸ­çš„æ–‡æœ¬
        df = df[df[output_column].str.len() >= self.min_length]

        # åˆ†è¯
        if tokenize:
            df[token_column] = df[output_column].apply(self.tokenize)

        logger.info(f"å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ•°æ® {len(df)} æ¡")

        return df

    def extract_keywords(
        self,
        text: str,
        top_k: int = 10,
        method: str = "tfidf"
    ) -> List[tuple]:
        """
        æå–å…³é”®è¯

        Args:
            text: è¾“å…¥æ–‡æœ¬
            top_k: è¿”å›å…³é”®è¯æ•°é‡
            method: æå–æ–¹æ³• (tfidf, textrank)

        Returns:
            å…³é”®è¯åˆ—è¡¨ [(word, weight), ...]
        """
        if not text:
            return []

        cleaned = self.clean(text)

        if method == "tfidf":
            import jieba.analyse
            keywords = jieba.analyse.extract_tags(
                cleaned, topK=top_k, withWeight=True
            )
        elif method == "textrank":
            import jieba.analyse
            keywords = jieba.analyse.textrank(
                cleaned, topK=top_k, withWeight=True
            )
        else:
            keywords = []

        return keywords


if __name__ == "__main__":
    # æµ‹è¯•
    cleaner = TextCleaner()

    test_texts = [
        "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼#åŒ—äº¬ç”Ÿæ´»# @å°æ˜ https://example.com [å¼€å¿ƒ]",
        "ç–«æƒ…é˜²æ§æ”¿ç­–è°ƒæ•´äº†ï¼Œå¤§å®¶æ€ä¹ˆçœ‹ï¼Ÿ",
        "ç»æµå½¢åŠ¿åˆ†æï¼šGDPå¢é•¿è¾¾åˆ°é¢„æœŸç›®æ ‡ ğŸ‘"
    ]

    for text in test_texts:
        print(f"\nåŸæ–‡: {text}")
        print(f"æ¸…æ´—: {cleaner.clean(text)}")
        print(f"åˆ†è¯: {cleaner.tokenize(text)}")

